{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clingo\n",
    "import ast\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from os.path import join\n",
    "from skimage import io\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import hamming_loss\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def atof(text):\n",
    "    try:\n",
    "        retval = float(text)\n",
    "    except ValueError:\n",
    "        retval = text\n",
    "    return retval\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    float regex comes from https://stackoverflow.com/a/12643073/190597\n",
    "    '''\n",
    "    return [ atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDigits(Dataset):\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with label annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.label_file = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir, str(idx) + '.jpg')\n",
    "        image = io.imread(img_name, as_gray=True)\n",
    "        label = self.label_file.iloc[idx, 1]\n",
    "        image = mnist_transform(image)\n",
    "\n",
    "        return image.float(), label - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self, num_out):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),  # 6 is the output chanel size; 5 is the kernal size; 1 (chanel) 28 28 -> 6 24 24\n",
    "            nn.MaxPool2d(2, 2),  # kernal size 2; stride size 2; 6 24 24 -> 6 12 12\n",
    "            nn.ReLU(True),       # inplace=True means that it will modify the input directly thus save memory\n",
    "            nn.Conv2d(6, 16, 5),  # 6 12 12 -> 16 8 8\n",
    "            nn.MaxPool2d(2, 2),  # 16 8 8 -> 16 4 4\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_out),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hitting_sets(min_dv, max_dv):\n",
    "    digits = list(range(min_dv, max_dv+1))\n",
    "    k_2_combos = list(itertools.combinations(digits, 2))\n",
    "    k_1_combos = [[d] for d in digits]\n",
    "    hs = []\n",
    "    for c in k_2_combos:\n",
    "        hs.append(list(c))\n",
    "    hs += k_1_combos\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_base = '../../../data/mnist_1_to_5/test'\n",
    "fashion_mnist_test_base = '../../../data/fashion_mnist_1_to_5/test'\n",
    "mnist_test_digits = MNISTDigits(join(mnist_test_base, 'labels.csv'), mnist_test_base)\n",
    "fashion_mnist_test_digits = MNISTDigits(join(fashion_mnist_test_base, 'labels.csv'), fashion_mnist_test_base)\n",
    "possible_hitting_sets = generate_hitting_sets(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_learned_hyp = '''\n",
    ":- ss_element(V1,V2); not hit(V1); elt(V2); ss(V1).\n",
    "0 {hs(V1,V2) } 1 :- elt(V2); hs_index(V1).\n",
    "hit(V1) :- hs(V3,V2); ss_element(V1,V2); hs_index(V3); elt(V2); ss(V1).\n",
    ":- hs(V3,V1); hs(V3,V2); V1 != V2; hs_index(V3); elt(V2); elt(V1).\n",
    "'''\n",
    "chs_learned_hyp = '''\n",
    ":- ss_element(3,V2); ss_element(V1,1); elt(V2); ss(V1).\n",
    ":- ss_element(V1,V2); not hit(V1); elt(V2); ss(V1).\n",
    "0 {hs(V1,V2) } 1 :- elt(V2); hs_index(V1).\n",
    "hit(V1) :- hs(V3,V2); ss_element(V1,V2); hs_index(V3); elt(V2); ss(V1).\n",
    ":- hs(V3,V1); hs(V3,V2); V1 != V2; hs_index(V3); elt(V2); elt(V1).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clingo(p):\n",
    "    clingo_control = clingo.Control([\"--warn=none\", '0', '--project'])\n",
    "    modls = []\n",
    "    try:\n",
    "        clingo_control.add(\"base\", [], p)\n",
    "    except RuntimeError:\n",
    "        print('Clingo runtime error')\n",
    "        print('Program: {0}'.format(p))\n",
    "        sys.exit(1)\n",
    "    clingo_control.ground([(\"base\", [])])\n",
    "\n",
    "    def on_model(m):\n",
    "        modls.append(str(m))\n",
    "\n",
    "    clingo_control.solve(on_model=on_model)\n",
    "    return modls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hs_from_clingo(elts, learned_hyp, ctx):\n",
    "    # Create and run clingo program to get hitting sets\n",
    "    clingo_prog = f'hs_index(1..2). ss(1..4). {elts} {learned_hyp}'\n",
    "    clingo_prog += f'\\n {ctx}\\n #show hs/2.'\n",
    "    models = run_clingo(clingo_prog)\n",
    "\n",
    "    # Process models\n",
    "    models_sets = []\n",
    "    for m in models:\n",
    "        # Ignore models with no hs(1,X).\n",
    "        facts = [i for i in m.split(' ') if 'hs(' in i]\n",
    "        if len(facts) != 1 or facts[0].split(',')[0].split('(')[1] == '1':\n",
    "            list_rep = []\n",
    "            for hsd in facts:\n",
    "                list_rep.append(int(hsd.split(',')[1].split(')')[0]))\n",
    "            if set(list_rep) not in models_sets:\n",
    "                models_sets.append(set(list_rep))\n",
    "\n",
    "    vector = [0]*len(possible_hitting_sets)\n",
    "    for phs_idx, phs in enumerate(possible_hitting_sets):\n",
    "        if set(phs) in models_sets:\n",
    "            vector[phs_idx] = 1\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(test_file, net, mnist_images, learned_hyp):\n",
    "    with open(test_file, 'r') as testf:\n",
    "        testf = testf.readlines()[1:]\n",
    "        hamming_losses = []\n",
    "        for line in testf:\n",
    "            ss, hitting_sets = line.rstrip().split('|')\n",
    "            ss = ast.literal_eval(ss)\n",
    "            nn_pred_ctx = ''\n",
    "            ground_truth_ctx = ''\n",
    "            ground_truth_elements = []\n",
    "            nn_pred_elements = []\n",
    "            for subset_id, subset in enumerate(ss):\n",
    "                nn_preds_this_subset = []\n",
    "                for image_id in subset:\n",
    "                    \n",
    "                    im = mnist_images[image_id]\n",
    "                    im_label = im[1] + 1\n",
    "                    \n",
    "                    # Ground truth\n",
    "                    ground_truth_ctx += f'ss_element({subset_id+1},{im_label}).\\n'\n",
    "                    ground_truth_elements.append(im_label)\n",
    "                    \n",
    "                    # Show image for debugging\n",
    "                    # plt.imshow(im[0].permute(1, 2, 0))\n",
    "                    # plt.show()\n",
    "                    \n",
    "                    # NN prediction\n",
    "                    pred = net(im[0].unsqueeze(0)).argmax(dim=1)[0].item() + 1\n",
    "                    nn_pred_ctx += f'ss_element({subset_id+1},{pred}).\\n'\n",
    "                    nn_pred_elements.append(pred)\n",
    "            \n",
    "            \n",
    "            gt_elt_str = ' '.join([f'elt({d}).' for d in ground_truth_elements])\n",
    "            pred_elt_str = ' '.join([f'elt({d}).' for d in nn_pred_elements])\n",
    "            \n",
    "            # Get true hs\n",
    "            true_hs = get_hs_from_clingo(gt_elt_str, learned_hyp, ground_truth_ctx)\n",
    "            \n",
    "            # Get predicted hs\n",
    "            predicted_hs = get_hs_from_clingo(pred_elt_str, learned_hyp, nn_pred_ctx)\n",
    "            \n",
    "            # Compute hamming loss\n",
    "            hl = hamming_loss(true_hs, predicted_hs)\n",
    "            hamming_losses.append(hl)\n",
    "\n",
    "        return np.mean(hamming_losses), sem(hamming_losses)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hamming_loss(example_dir, learned_hyp, dataset='HS_mnist'):\n",
    "    nsl_repeats_dir = example_dir+'/saved_results'\n",
    "    \n",
    "    if dataset is not None:\n",
    "        nsl_repeats_dir = nsl_repeats_dir+'/repeats/'+dataset\n",
    "    else:\n",
    "        nsl_repeats_dir = nsl_repeats_dir+'/repeats'\n",
    "    \n",
    "    # We get mean hamming loss after 20 epochs\n",
    "    hamming_losses = []\n",
    "    nsl_dir = nsl_repeats_dir+'/'+str(100)\n",
    "    repeats = os.listdir(nsl_dir)\n",
    "    repeats = [r for r in repeats if r != '.DS_Store']\n",
    "    repeats.sort(key=natural_keys)\n",
    "    \n",
    "    for idx, i in enumerate(repeats):\n",
    "        # Read network\n",
    "        network_weights_path = nsl_dir+'/'+i+'/networks/net_digit_iteration_20.pt'\n",
    "        net = MNISTNet(5)\n",
    "        net.load_state_dict(torch.load(network_weights_path, map_location=torch.device('cpu')))\n",
    "        net.eval()\n",
    "\n",
    "        # Get NN predictions for each image on test set\n",
    "        if 'HS' in dataset:\n",
    "            task = 'hs'\n",
    "        else:\n",
    "            task = 'chs'\n",
    "        if 'mnist' in dataset:\n",
    "            images = mnist_test_digits\n",
    "            test_file = example_dir + f'data/{task}/mnist_test_with_hs.csv'\n",
    "        else:\n",
    "            images = fashion_mnist_test_digits\n",
    "            test_file = example_dir + f'data/{task}/fashion_mnist/test_with_hs.csv'\n",
    "        hl, _ = run(test_file, net, images, learned_hyp)\n",
    "        hamming_losses.append(hl)\n",
    "        \n",
    "    # Compute mean and std err across all repeats\n",
    "    hamming_loss_mean = np.mean(hamming_losses)\n",
    "    hamming_loss_err =  sem(hamming_losses)\n",
    "        \n",
    "    return hamming_loss_mean, hamming_loss_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "example_dir = '../../../examples/hitting_sets/'\n",
    "lh = hs_learned_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002985232067510549, 0.0001943696188940777)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_hamming_loss(example_dir, lh, dataset='HS_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14970464135021097, 0.0007783818553406404)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_hamming_loss(example_dir, lh, dataset='HS_fashion_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "chs_lh = chs_learned_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002721518987341772, 0.00012611877535556893)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_hamming_loss(example_dir, chs_lh, dataset='CHS_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12614978902953583, 0.002098039870738607)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_hamming_loss(example_dir, chs_lh, dataset='CHS_fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
